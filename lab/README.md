# PGVector RAG Search Lab

A modular, production-ready implementation of Retrieval-Augmented Generation (RAG) using PostgreSQL's pgvector extension. This lab demonstrates advanced vector similarity search techniques across Wikipedia articles and movie/Netflix datasets.

## ğŸš€ Quick Start

### Prerequisites
- PostgreSQL 15+ with pgvector extension
- Python 3.11+
- OpenAI API key

### Installation

1. **Clone and setup environment:**
```bash
git clone <repository>
cd Movies_pgvector_lab
python -m venv pgvector_lab
source pgvector_lab/bin/activate  # On Windows: pgvector_lab\Scripts\activate
```

2. **Install dependencies:**
```bash
pip install psycopg2-binary openai pgvector transformers torch streamlit fastapi uvicorn plotly pandas
```

3. **Configure environment:**
```bash
export DATABASE_URL="postgresql://username:password@localhost/database"
export OPENAI_API_KEY="your_openai_api_key"
```

4. **Generate embeddings:**
```bash
# Wikipedia embeddings
python lab/03_embeddings/generate_embeddings.py --source wikipedia --type dense

# Netflix embeddings  
python lab/03_embeddings/generate_embeddings.py --source movies --type dense --include-netflix
```

5. **Start the web interface:**
```bash
streamlit run lab/05_api/streamlit_app.py
```

## ğŸ—ï¸ Architecture Overview

### Core Services (`lab/core/`)
- **DatabaseService**: Connection pooling, pgvector registration, query helpers
- **EmbeddingService**: Dense (OpenAI) and sparse (SPLADE) embedding generation
- **SearchService**: Vector similarity search with multiple strategies
- **RankingService**: Result merging using RRF and weighted combination
- **GenerationService**: LLM-based answer generation with cost tracking
- **ConfigService**: Centralized configuration management

### Data Processing (`lab/02_data/`)
- **Smart chunking**: Fixed-size, semantic, hierarchical, and adaptive strategies
- **Data loaders**: Wikipedia and Movie/Netflix dataset integration
- **Text processing**: Cleaning, validation, deduplication, metadata extraction

### Embedding Generation (`lab/03_embeddings/`)
- **Batch processing**: Efficient embedding generation with progress tracking
- **CLI tools**: Command-line interfaces for generation and verification
- **Quality assurance**: Comprehensive embedding validation and statistics

### Search Implementations (`lab/04_search/`)
- **Simple Search**: Basic dense/sparse vector similarity
- **Hybrid Search**: Configurable weighted combination of dense + sparse
- **Adaptive Search**: Query classification with dynamic weight adjustment

### API & UI (`lab/05_api/`)
- **FastAPI**: RESTful API with automatic documentation
- **Streamlit**: Interactive web interface with real-time search comparison
- **Metrics**: Performance tracking and visualization

## ğŸ” Search Methods

### 1. Dense Vector Search
Uses OpenAI text-embedding-3-small (1536 dimensions) for semantic similarity.
```python
from lab.04_search.simple_search import SimpleSearchEngine

engine = SimpleSearchEngine(db_service, config, 'wikipedia')
results = engine.search_dense("What is machine learning?", top_k=10)
```

### 2. Sparse Vector Search  
Uses SPLADE model for keyword-based sparse representations.
```python
results = engine.search_sparse("machine learning algorithms", top_k=10)
```

### 3. Hybrid Search
Combines dense and sparse with configurable weights.
```python
from lab.04_search.hybrid_search import HybridSearchEngine

engine = HybridSearchEngine(db_service, config, 'wikipedia', 
                           dense_weight=0.7, sparse_weight=0.3)
results = engine.search_hybrid("machine learning", top_k=10)
```

### 4. Adaptive Search
Automatically classifies queries and adjusts weights:
- **Factual queries** (0.3 dense, 0.7 sparse): "When was Python created?"
- **Conceptual queries** (0.7 dense, 0.3 sparse): "How does machine learning work?"
- **Exploratory queries** (0.5 dense, 0.5 sparse): "Tell me about artificial intelligence"

```python
from lab.04_search.adaptive_search import AdaptiveSearchEngine

engine = AdaptiveSearchEngine(db_service, config, 'wikipedia')
results = engine.search_adaptive("What is the capital of France?", top_k=10)
```

## ğŸ› ï¸ Usage Examples

### Command Line Interface

**Generate embeddings:**
```bash
# Dense embeddings for Wikipedia
python lab/03_embeddings/generate_embeddings.py --source wikipedia --type dense --limit 1000

# Sparse embeddings for Netflix shows
python lab/03_embeddings/generate_embeddings.py --source movies --type sparse --include-netflix

# Verify embedding quality
python lab/03_embeddings/verify_embeddings.py --source wikipedia --detailed --samples 5
```

**Interactive search:**
```bash
# Simple search
python lab/04_search/simple_search.py --source wikipedia --interactive

# Hybrid search with custom weights
python lab/04_search/hybrid_search.py --source wikipedia --interactive --dense-weight 0.7 --sparse-weight 0.3

# Adaptive search with query analysis
python lab/04_search/adaptive_search.py --source wikipedia --interactive
```

**Compare search methods:**
```bash
python lab/04_search/hybrid_search.py --source wikipedia --query "machine learning" --compare-methods
```

### Web Interfaces

**Streamlit UI:**
```bash
streamlit run lab/05_api/streamlit_app.py
```

**FastAPI Server:**
```bash
python lab/05_api/fastapi_server.py
# API documentation: http://localhost:8000/docs
```

### Python API

```python
from lab.core.database import DatabaseService
from lab.core.config import ConfigService
from lab.04_search.adaptive_search import AdaptiveSearchEngine

# Initialize services
config = ConfigService()
db_service = DatabaseService(config.database.connection_string)
engine = AdaptiveSearchEngine(db_service, config, 'wikipedia')

# Perform search
results = engine.search_adaptive("What is quantum computing?", top_k=5)

# Generate answer
response = engine.generate_adaptive_answer("Explain machine learning", top_k=10)
print(response['answer'])
```

## ğŸ“Š Features

### Advanced Search Capabilities
- **Query Classification**: Automatic detection of factual vs conceptual questions
- **Hybrid Ranking**: Reciprocal Rank Fusion (RRF) and weighted combination
- **Context Optimization**: Smart content chunking and token management
- **Cost Tracking**: OpenAI usage monitoring and cost estimation

### Production Ready
- **Connection Pooling**: Efficient database resource management
- **Error Handling**: Comprehensive retry logic and graceful degradation  
- **Progress Tracking**: Real-time feedback for long-running operations
- **Logging**: Structured logging with configurable levels

### Evaluation & Monitoring
- **Performance Metrics**: Query latency, result quality, cost analysis
- **Search Comparison**: Side-by-side evaluation of different methods
- **Embedding Validation**: Quality checks and statistical analysis
- **Interactive Visualization**: Real-time charts and performance dashboards

## ğŸ—‚ï¸ Project Structure

```
lab/
â”œâ”€â”€ core/                      # Core service layer
â”‚   â”œâ”€â”€ database.py           # Database service with connection pooling
â”‚   â”œâ”€â”€ embeddings.py         # Dense & sparse embedding services
â”‚   â”œâ”€â”€ search.py             # Vector search implementations
â”‚   â”œâ”€â”€ ranking.py            # Result merging and re-ranking
â”‚   â”œâ”€â”€ generation.py         # LLM answer generation
â”‚   â””â”€â”€ config.py             # Configuration management
â”‚
â”œâ”€â”€ 02_data/                   # Data processing utilities
â”‚   â”œâ”€â”€ processor.py          # Text cleaning and validation
â”‚   â”œâ”€â”€ chunking.py           # Smart document chunking
â”‚   â””â”€â”€ loaders.py            # Data source connectors
â”‚
â”œâ”€â”€ 03_embeddings/             # Embedding generation tools
â”‚   â”œâ”€â”€ embedding_manager.py  # Orchestration layer
â”‚   â”œâ”€â”€ generate_embeddings.py # CLI for batch generation
â”‚   â””â”€â”€ verify_embeddings.py  # Quality verification
â”‚
â”œâ”€â”€ 04_search/                 # Search implementations
â”‚   â”œâ”€â”€ simple_search.py      # Basic dense/sparse search
â”‚   â”œâ”€â”€ hybrid_search.py      # Weighted combination search
â”‚   â””â”€â”€ adaptive_search.py    # Query-aware adaptive search
â”‚
â”œâ”€â”€ 05_api/                    # Web interfaces
â”‚   â”œâ”€â”€ fastapi_server.py     # REST API backend
â”‚   â””â”€â”€ streamlit_app.py      # Interactive web UI
â”‚
â””â”€â”€ 07_evaluation/             # Performance evaluation
    â”œâ”€â”€ benchmark.py          # Performance benchmarking
    â””â”€â”€ metrics.py            # Evaluation metrics
```

## ğŸ¯ Use Cases

### Research & Education
- **Vector search comparison**: Evaluate different embedding strategies
- **RAG system prototyping**: Rapid development of retrieval systems
- **Conference presentations**: Ready-to-demo search capabilities

### Production Applications  
- **Knowledge bases**: Enterprise document search and Q&A
- **Content recommendation**: Movie/show recommendation systems
- **Customer support**: Automated response generation

### Development & Testing
- **Algorithm benchmarking**: Performance comparison frameworks
- **Data quality assessment**: Embedding validation and statistics
- **Cost optimization**: Token usage and API cost analysis

## ğŸ”§ Configuration

### Environment Variables
```bash
# Database
DATABASE_URL="postgresql://user:pass@localhost/db"

# OpenAI
OPENAI_API_KEY="your_openai_api_key"

# Models
OPENAI_EMBEDDING_MODEL="text-embedding-3-small"
SPLADE_MODEL="naver/splade-cocondenser-ensembledistil"

# Search weights
DENSE_WEIGHT=0.5
SPARSE_WEIGHT=0.5

# API
API_HOST="0.0.0.0"
API_PORT=8000
```

### Configuration File (JSON)
```json
{
  "database": {
    "connection_string": "postgresql://user:pass@localhost/db",
    "min_connections": 1,
    "max_connections": 20
  },
  "embedding": {
    "openai_model": "text-embedding-3-small",
    "splade_model": "naver/splade-cocondenser-ensembledistil",
    "batch_size_dense": 50,
    "batch_size_sparse": 5
  },
  "generation": {
    "model": "gpt-4o",
    "temperature": 0.7,
    "max_tokens": 1000
  }
}
```

## ğŸ“ˆ Performance

### Benchmarks (Approximate)
- **Query latency**: <100ms (P95) for hybrid search
- **Embedding generation**: 100-200 items/minute (dense), 20-50 items/minute (sparse)
- **Memory usage**: <2GB for standard operations
- **Cost**: ~$0.002-0.005 per query (including embeddings and generation)

### Optimization Tips
- Use connection pooling for concurrent operations
- Batch embedding generation for efficiency
- Cache frequently accessed embeddings
- Monitor token usage for cost control

## ğŸ¤ Contributing

1. **Follow the modular architecture**: Keep services independent and reusable
2. **Add comprehensive tests**: Include unit tests for new components  
3. **Document thoroughly**: Update README and docstrings
4. **Performance focus**: Maintain sub-second query response times

## ğŸ“ License

This project is part of the PostgreSQL pgvector ecosystem and follows the same open-source principles. Use freely for research, education, and production applications.

## ğŸ™ Acknowledgments

- **pgvector**: The PostgreSQL vector similarity extension
- **OpenAI**: Embedding and generation models
- **SPLADE**: Sparse lexical and expansion model
- **PostgreSQL**: The world's most advanced open source database
- **Community contributors**: Everyone who helps improve vector search

---

**Ready to explore advanced vector search?** Start with the Quick Start guide above and dive into the world of semantic similarity! ğŸš€